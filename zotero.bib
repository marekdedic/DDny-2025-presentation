
@inproceedings{prochazka_which_2023,
	location = {Tatranské Matliare, Slovakia},
	title = {Which Graph Properties Affect {GNN} Performance for a Given Downstream Task?},
	volume = {3498},
	rights = {All rights reserved},
	url = {https://ceur-ws.org/Vol-3498/#paper7},
	series = {{CEUR} Workshop Proceedings},
	abstract = {Machine learning algorithms on graphs, in particular graph neural networks, became a popular framework for solving various tasks on graphs, attracting significant interest in the research community in recent years. As presented, however, these algorithms usually assume that the input graph is fixed and well-defined and do not consider the problem of constructing the graph for a given practical task. This work proposes a methodical way of linking graph properties with the performance of a {GNN} solving a given task on such graph via a surrogate regression model that is trained to predict the performance of the {GNN} from the properties of the graph dataset. Furthermore, the {GNN} model hyper-parameters are optionally added as additional features of the surrogate model and it is shown that this technique can be used to solve the practical problem of hyper-parameter tuning. We experimentally evaluate the importance of graph properties as features of the surrogate model with regards to the node classification task for several common graph datasets and discuss how these results can be used for graph composition tailored to the given task. Finally, our experiments indicate a significant gain in the proposed hyper-parameter tuning method compared to the reference grid-search method.},
	eventtitle = {Information Technologies – Applications and Theory 2023},
	pages = {58--66},
	booktitle = {Proceedings of the 23nd Conference Information Technologies – Applications and Theory ({ITAT} 2023)},
	publisher = {{CEUR}-{WS}.org},
	author = {Procházka, Pavel and Mareš, Michal and Dědič, Marek},
	urldate = {2023-09-30},
	date = {2023-10-07},
	langid = {english},
}

@online{kaiku_hyperparameter_2023,
	title = {Hyperparameter Tuning},
	url = {https://medium.com/@kaikuh/hyperparameter-tuning-7b279266d239},
	abstract = {Optimizing hyperparameters is a crucial step in building a robust neural network model. In this blog post, we will explore the use of…},
	titleaddon = {Medium},
	author = {kaiku},
	urldate = {2025-04-11},
	date = {2023-05-04},
	langid = {english},
	file = {Snapshot:/home/madedic/Zotero/storage/ID5E3LJU/hyperparameter-tuning-7b279266d239.html:text/html},
}

@article{bischl_hyperparameter_2023,
	title = {Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges},
	volume = {13},
	rights = {© 2023 The Authors. {WIREs} Data Mining and Knowledge Discovery published by Wiley Periodicals {LLC}.},
	issn = {1942-4795},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1484},
	doi = {10.1002/widm.1484},
	shorttitle = {Hyperparameter optimization},
	abstract = {Most machine learning algorithms are configured by a set of hyperparameters whose values must be carefully chosen and which often considerably impact performance. To avoid a time-consuming and irreproducible manual process of trial-and-error to find well-performing hyperparameter configurations, various automatic hyperparameter optimization ({HPO}) methods—for example, based on resampling error estimation for supervised machine learning—can be employed. After introducing {HPO} from a general perspective, this paper reviews important {HPO} methods, from simple techniques such as grid or random search to more advanced methods like evolution strategies, Bayesian optimization, Hyperband, and racing. This work gives practical recommendations regarding important choices to be made when conducting {HPO}, including the {HPO} algorithms themselves, performance evaluation, how to combine {HPO} with machine learning pipelines, runtime improvements, and parallelization. This article is categorized under: Algorithmic Development {\textgreater} Statistics Technologies {\textgreater} Machine Learning Technologies {\textgreater} Prediction},
	pages = {e1484},
	number = {2},
	journaltitle = {{WIREs} Data Mining and Knowledge Discovery},
	author = {Bischl, Bernd and Binder, Martin and Lang, Michel and Pielok, Tobias and Richter, Jakob and Coors, Stefan and Thomas, Janek and Ullmann, Theresa and Becker, Marc and Boulesteix, Anne-Laure and Deng, Difan and Lindauer, Marius},
	urldate = {2025-04-12},
	date = {2023},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1484},
	keywords = {automl, hyperparameter optimization, machine learning, model selection, tuning},
	file = {Full Text PDF:/home/madedic/Zotero/storage/VT3KCMMC/Bischl et al. - 2023 - Hyperparameter optimization Foundations, algorithms, best practices, and open challenges.pdf:application/pdf;Snapshot:/home/madedic/Zotero/storage/DXRWP49S/widm.html:text/html},
}
