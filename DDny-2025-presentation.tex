\documentclass[10pt, aspectratio=169]{beamer}
\setbeamertemplate{navigation symbols}{}

\usepackage{polyglossia}

\usepackage{adjustbox}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{appendixnumberbeamer}
\usepackage[backend=biber,style=iso-authoryear,sortlocale=en_US,autolang=other,bibencoding=UTF8]{biblatex}
\usepackage{booktabs}
\usepackage{color}
\usepackage{csquotes}
\usepackage{datetime}
\usepackage{fontspec}
\usepackage{graphics}
\usepackage{hyperref}
\usepackage{makecell}
\usepackage{mathrsfs}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{pifont}
\usepackage[mode=buildnew]{standalone}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{threeparttable}
\usepackage{todonotes}
\usepackage{ulem}
\usepackage{url}

\addbibresource{zotero.bib}

\setdefaultlanguage{english}
\setotherlanguage{czech}
\setmainfont{TeX Gyre Termes}
\usetheme{Boadilla}
\usecolortheme{crane}
\setbeamertemplate{title page}[default][rounded=true,shadow=false]
\setbeamertemplate{section in toc}[ball unnumbered]
\setbeamertemplate{bibliography item}{}

\hypersetup{%
	pdfencoding=auto,
	unicode=true,
	citecolor=green,
	filecolor=blue,
	linkcolor=red,
	urlcolor=blue
}

\makeatletter
\newcommand*{\currentSection}{\@currentlabelname}
\makeatother

\def\mathdefault#1{#1}

\input{notation}

\title[RL for structured data]
{%
	Representation learning for structured data
}

\newdate{presentation}{25}{11}{2025}
\date[October 2025]{FNSPE Doctoral days, \displaydate{presentation}}

\author[Marek Dědič]
{%
	Marek~Dědič\inst{1, 2}
}

\institute[CTU in Prague, Cisco]
{%
	\inst{1} Czech Technical University in Prague \and
	\inst{2} Cisco Systems, Inc.
}

\begin{document}

\begin{frame}
	\titlepage
\end{frame}

\begin{frame}{Parameters \& hyper-parameters}
	\begin{itemize}
		\item Learners have parameters and hyperparameters
		\item Parameters optimized automatically
		\item Hyper-parameters set by humans
	\end{itemize}
	\centering
	\includegraphics[width=0.5\linewidth]{images/HPO.png}\footcite{kaiku_hyperparameter_2023}
\end{frame}

\begin{frame}{GNN hyper-parameter setups}
	\centering
	\only<1>{\includestandalone[width=\linewidth, page = 1]{images/hyperparameter-setups/hyperparameter-setups}}
	\only<2>{\includestandalone[width=\linewidth, page = 2]{images/hyperparameter-setups/hyperparameter-setups}}
	\only<3>{\includestandalone[width=\linewidth, page = 3]{images/hyperparameter-setups/hyperparameter-setups}}
\end{frame}

\begin{frame}{Hyper-parameter optimization}
	\begin{itemize}
		\item Hyper-parameters need to be found (semi-)automatically
		\item No gradients \( \rightarrow \) black-box problem
		\item \textit{Expensive} black-box problem
	\end{itemize}
\end{frame}

\begin{frame}{HPO loop}
	\centering
	\includegraphics[width=0.8\linewidth]{images/HPO-loop.pdf}\footcite{bischl_hyperparameter_2023}
\end{frame}

\begin{frame}{Hyper-parameter optimization}
	\textbf{Goals \& contributions}:
	\begin{itemize}
		\item Evaluate standard HPO algorithms for GNNs
		\item Evaluate a Cross-RF HPO algorithm specifically for GNNs\footnote{Building on previous work \cite{prochazka_which_2023}}
	\end{itemize}
\end{frame}

\begin{frame}{Evaluated standard algorithms}
	\begin{itemize}
		\item Grid-search
		\item Random-search
		\item Gaussian-process-based optimization
		\item Tree-structured Parzen Estimator
		\item Quasi-Monte Carlo
	\end{itemize}
\end{frame}

\begin{frame}{Cross-RF approach}
	\begin{itemize}
		\item A \textbf{meta-model}: Predict model performance from its hyper-parameters and dataset properties.
		\item Use for HPO -- selecting the next hyper-parameter configuration \( \hat{\lambda} \):
			\[ \hat{\lambda} = \argmax_{\lambda \in \tilde{\Lambda}} \left( \textrm{meta-model} \left( \lambda \right) \right) \]
		\item Meta-model is cheap \rightarrow \, \( \argmax \) is feasible.
		\item Meta-model can be \textbf{pre-trained} on other similar datasets.
	\end{itemize}
\end{frame}

\begin{frame}{Meta-dataset}
	A meta-dataset used for the pre-training:
	\begin{center}
		\resizebox{\linewidth}{!}{%
			\begin{tabular}{rrrrrrrr}
				\toprule
				\multicolumn{4}{c}{\textbf{Dataset properties}} & \multicolumn{3}{c}{\textbf{Hyper-parameters}} & \textbf{Perf.\ metrics} \\
				\cmidrule(r){1-4} \cmidrule(lr){5-7} \cmidrule(l){8-8}
				\textbf{Node count} & \textbf{Class ratio} & \textbf{\# of components} & \textbf{Avg.\ node degree} & \textbf{\# of layers} & \textbf{Dropout} & \textbf{Activation} & \textbf{F1} \\
				\midrule
				6 & 1 / 3 & 1 & 2 & 2 & 0.3 & ReLU & 0.65 \\
				6 & 1 / 3 & 1 & 2 & 3 & 0.3 & ReLU & 0.7 \\
				6 & 1 / 3 & 1 & 2 & 2 & 0.5 & ReLU & 0.62 \\
				6 & 1 / 3 & 1 & 2 & 3 & 0.5 & ReLU & 0.6 \\
				\bottomrule
			\end{tabular}
		}
	\end{center}
\end{frame}

\begin{frame}{Experiment setup}
	\begin{itemize}
		\item 10 standard datasets (34 nodes -- 169k nodes)
		\item GraphSAGE model with 8 hyper-parameters
		\item 10 repeated iterations for each configuration
		\item HPO early-stopping based on wall-clock time \& improvement
		\item Used a random forrest regressor as the meta-model
	\end{itemize}
\end{frame}


\begin{frame}{Benchmark results}
	\begin{columns}[t,onlytextwidth]
		\begin{column}{0.45\textwidth}
			\vspace{1cm}
			\begin{itemize}
				\item Bayesian methods\,(GP \& TPE) confirmed their advantage
				\item Surprisingly, QMC scored the lowest on 6 out of 10 datasets
			\end{itemize}
		\end{column}

		\begin{column}{0.55\textwidth}
			\centering
			\scriptsize
			\begin{table}
				\centering
				\begin{adjustbox}{max width=\linewidth}
					\begin{tabular}{lrrrrr}
						\toprule
						\textbf{Dataset} & \textbf{random} & \textbf{gp} & \textbf{tpe} & \textbf{grid} & \textbf{qmc} \\
						\midrule
						ArXiv        & 0.4261 & 0.4335 & \textbf{0.4354} & 0.4251 & 0.4244 \\
						CiteSeer     & 0.7341 & 0.7380 & \textbf{0.7485} & 0.7341 & 0.7375 \\
						Computers    & 0.9099 & 0.9112 & \textbf{0.9123} & 0.9109 & 0.9088 \\
						Cora         & 0.8817 & \textbf{0.8892} & 0.8879 & 0.8811 & 0.8786 \\
						CoraFull     & 0.6638 & 0.6664 & \textbf{0.6691} & 0.6637 & 0.6625 \\
						DBLP         & 0.8187 & \textbf{0.8209} & 0.8194 & 0.8175 & 0.8209 \\
						Flickr       & 0.1987 & \textbf{0.2142} & 0.2057 & 0.2059 & 0.1885 \\
						KarateClub   & 0.8688 & \textbf{0.9748} & 0.9534 & 0.9510 & 0.8497 \\
						PubMed       & 0.8841 & 0.8900 & 0.8900 & 0.8892 & \textbf{0.8907} \\
						Squirrel     & \textbf{0.4037} & 0.4013 & 0.3972 & 0.4004 & 0.3904 \\
						\bottomrule
					\end{tabular}
				\end{adjustbox}
				\caption{Mean final F1 per algorithm and dataset.}
			\end{table}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Example on the CiteSeer dataset}
	\begin{figure}
		\centering
		%\includegraphics[width=0.8\linewidth]{images/CiteSeer_convergence.png}
		\caption{Comparison between all samplers}
	\end{figure}
\end{frame}

\begin{frame}{Meta-model results}
	\begin{columns}[t,onlytextwidth]
		\begin{column}{0.45\textwidth}
			\vspace{1cm}
			\begin{itemize}
				\item Compared with grid search and GP
				\item Improved over GP on 4 out of 10 datasets
				\item When worse than GP, it trails closely behind
			\end{itemize}
		\end{column}

		\begin{column}{0.4\textwidth}
			\begin{table}[htbp]
				\centering
				\begin{adjustbox}{max width=\linewidth}
					\begin{tabular}{lrrr}
						\toprule
						Dataset     & Grid     & GP   & Cross‐RF \\
						\midrule
						ArXiv       & 0.4251  & 0.4335  & \textbf{0.4378}   \\
						CiteSeer    & 0.7341  & \textbf{0.7380}  & 0.7379   \\
						Computers   & 0.9109  & \textbf{0.9112}  & \textbf{0.9121}   \\
						Cora        & 0.8811  & \textbf{0.8892}  & 0.8828   \\
						CoraFull    & 0.6637  & \textbf{0.6664}  & 0.6616   \\
						DBLP        & 0.8175  & 0.8209  & \textbf{0.8212}   \\
						Flickr      & 0.2059  & \textbf{0.2142}  & 0.2134   \\
						KarateClub  & 0.9510  & \textbf{0.9748}  & 0.8205   \\
						PubMed      & 0.8892  & \textbf{0.8900}  & 0.8899   \\
						Squirrel    & 0.4004  & 0.4013  & \textbf{0.4044}   \\
						\bottomrule
					\end{tabular}
				\end{adjustbox}
				\caption{Mean final F1 score by algorithm and dataset.}
			\end{table}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Example on the DBLP dataset}
	\begin{figure}
		\centering
		%\includegraphics[width=0.8\linewidth]{images/DBLP_mean_per_trial.png}
		\caption{Comparison with two other samplers}
	\end{figure}
\end{frame}

\begin{frame}{Next steps}
	\begin{itemize}
		\item More performance metrics
		\item Try different meta-model architectures
		\item Try to optimize accross GNN architectures
		\item Training on synthetic graphs
		\item Incorporate as surrogate model for Bayesian HPO
	\end{itemize}
\end{frame}

\begin{frame}{Conclusion}
	\begin{itemize}
		\item Evaluated standard HPO algorithms for GNNs
		\item Shown the Cross-RF hyper-parameter optimization algorithm for GNNs to be comparable to SoTA
		\item Implemented as a standard Optuna sampler
		\item More interesting research to come
	\end{itemize}
\end{frame}

\begin{frame}
	\titlepage
\end{frame}

\appendix

\begin{frame}[allowframebreaks]{Bibliography}
	\printbibliography
\end{frame}

\begin{frame}
	\begin{table}[ht]
		\begin{tabular}{lrrrr}
			\toprule
			Dataset     & \#Nodes       & \#Edges             & \#Features            & \#Classes \\
			\midrule
			KarateClub  & 34                 & 156            & 34                    & 4               \\
			Cora        & 2\,708             & 10\,556        & 1\,433                & 7               \\
			CiteSeer    & 3\,327             & 9\,104         & 3\,703                & 6               \\
			PubMed      & 19\,717            & 88\,648        & 500                   & 3               \\
			ArXiv       & 169\,343           & 1\,166\,243    & 128                   & 40              \\
			DBLP        & 17\,716            & 105\,734       & 1\,639                & 4               \\
			CoraFull    & 19\,793            & 126\,842       & 8\,710                & 70              \\
			Flickr      & 89\,250            & 899\,756       & 500                   & 7               \\
			Computers   & 13\,752            & 491\,722       & 767                   & 10              \\
			Squirrel    & 5\,201             & 198\,493       & 3\,169                & -               \\
			\bottomrule
		\end{tabular}
	\end{table}
\end{frame}

\begin{frame}
	\begin{table}[ht]
		\centering
		\footnotesize
		\setlength{\tabcolsep}{4pt}
		\renewcommand{\arraystretch}{0.75}

		\resizebox{\linewidth}{!}{%
			\begin{threeparttable}
				\begin{tabular}{l c c c c c c c c c c}
					\toprule
					\textbf{Dataset} & \textbf{Ep} & \textbf{Lyrs} & \textbf{HW} & \textbf{Act} & \textbf{Opt} & \textbf{Drop} & \textbf{Agg} & \textbf{ES} & \textbf{L2} & \textbf{LR} \\
					\midrule
					\multicolumn{11}{l}{\itshape Small datasets}\\
					\addlinespace
					KarateClub & 500 & 1--2 & 16--32 &
					\multirow{4}{*}{$\substack{\text{ReLU}\\\text{PReLU}}$} &
					\multirow{4}{*}{$\substack{\text{Adam}\\\text{AdamW}}$} &
					\multirow{4}{*}{$\substack{0 \\ 0.5}$} &
					\multirow{4}{*}{$\substack{\text{Mean}\\\text{Max}}$} & 10 & 0, 1e-3, 1e-2 & 1-e2, 1e-3, 5e-4 \\

					Cora & 200 & 1--3 & 16--64 & & & & & 10 & \multirow{3}{*}{0, 5e-4, 5e-3} & \multirow{3}{*}{1e-2, 5e-3, 1e-3} \\
					CiteSeer & 200 & 1--3 & 16--64 & & & & & 10 &  &  \\
					Squirrel & 500 & 1--3 & 32--128 & & & & & 20 &  &  \\

					\addlinespace
					\multicolumn{11}{l}{\itshape Medium datasets}\\
					\addlinespace
					PubMed & 200 & 1--3 & 16--64 &
					\multirow{4}{*}{$\substack{\text{ReLU}\\\text{PReLU}}$} &
					\multirow{4}{*}{$\substack{\text{Adam}\\\text{AdamW}}$} &
					\multirow{4}{*}{$\substack{0 \\ 0.5}$} &
					\multirow{4}{*}{$\substack{\text{Mean}\\\text{Max}}$} & 10 & 0, 5e-4, 5e-3 & 1e-2, 5e-3, 1e-3 \\

					CoraFull & 500 & 2--3 & 64--128 & & & & & 20 & \multirow{3}{*}{0, 5e-5, 5e-4} & \multirow{3}{*}{1e-2, 5e-3, 1e-3} \\
					DBLP & 500 & 2--3 & 64--128 & & & & & 20 &  & \\
					Computers & 500 & 2--3 & 64--128 & & & & & 20 &  &  \\
					Flickr & 100 & 2--3 & 128--256 & & & & & 10 & 0, 5e-4 & 5e-3, 1e-3 \\

					\addlinespace
					\multicolumn{11}{l}{\itshape Large datasets}\\
					\addlinespace
					ArXiv & 150 & 2--3 & 256--512 &
					$\substack{\text{ReLU}\\\text{PReLU}}$ &
					$\substack{\text{Adam}\\\text{AdamW}}$ &
					$\substack{0 \\ 0.5}$ &
					$\substack{\text{Mean}\\\text{Max}}$ & 10 & 0, 5e-4 & 1e-2, 5e-3 \\
					\bottomrule
				\end{tabular}

				\begin{tablenotes}
					\item[*] \footnotesize\textbf{Abbreviations}: \textbf{Ep}: Epochs, \textbf{Lyrs}: Hidden Layers, \textbf{HW}: Hidden Width, \textbf{Act}: Activation Fn., \textbf{Opt}: Optimizer, \textbf{Drop}: Dropout Rate, \textbf{Agg}: Aggregation Fn., \textbf{ES}: Early Stopping at the model level, \textbf{L2}: L2 Regularization, \textbf{LR}: Learning Rate.
				\end{tablenotes}
			\end{threeparttable}
		}
	\end{table}
\end{frame}

\begin{frame}
	\begin{table}[ht]
		\centering
		\scriptsize
		\renewcommand{\arraystretch}{0.70}
		\resizebox{0.5\linewidth}{!}{%
			\begin{tabularx}{\textwidth}{p{3.7cm} c c c X}
				\toprule
				\makecell[l]{\textbf{Graph Dataset}\\\textbf{Property}}
				& \multicolumn{3}{c}{\textbf{Awareness}}
				& \textbf{Description / Definition} \\
				\cmidrule(lr){2-4}
				& \textbf{Task} & \textbf{Structure} & \textbf{Attributes} & \\
				\midrule
				\textbf{Node count}
				& No  & No  & No
				& Number of nodes — dataset size. \\
				% \textbf{Class ratio}
				%   & Yes & No  & No
				%   & Ratio between the number of positive and negative nodes. \\
				\textbf{Number of components}
				& No  & Yes & No
				& Number of connected components of the graph. \\
				\textbf{Average node degree}
				& No  & Yes & No
				& Average node degree in the graph. \\
				\textbf{Global assortativity}
				& No  & Yes & No
				& Measure of the tendency of nodes to connect with other similar nodes, rather than dissimilar nodes~[9]. \\
				\textbf{Attribute similarity}
				& No  & Yes & Yes
				& Average cosine similarity of attributes across all edges in the graph. \\
				\textbf{Attribute homophily}
				& Yes & Yes & No
				& Measure of how clustered together are nodes with similar attributes. \\
				\textbf{Edge homophily}
				& Yes & Yes & No
				& Fraction of edges connecting nodes of the same class. \\
				\textbf{Node homophily}
				& Yes & Yes & No
				& Fraction of node neighbours having the same class as the node in question, averaged over all nodes. \\
				\textbf{Class homophily}
				& Yes & Yes & No
				& A modification of node homophily that is invariant to the number of classes. \\
				\textbf{Ratio of positive nodes of degree $>1$}
				& Yes & Yes & No
				& Ratio of positive nodes with degree greater than one. \\
				\textbf{Fraction of positive nodes of degree $>2$}
				& Yes & Yes & No
				& The fraction of positive nodes with degree greater than two, out of those with degree greater than one. \\
				\textbf{Average positive node degree}
				& Yes & Yes & No
				& Average node degree in the sub-graph restricted to nodes from $V^1$. \\
				\textbf{Relative presence of positive edges}
				& Yes & Yes & No
				& Number of edges connecting positive nodes, divided by the number of edges that would be present in a theoretical clique constructed of all positive nodes. \\
				\textbf{Positive attribute similarity}
				& Yes & No  & Yes
				& Avg.\ feature similarity within positive class. \\
				\textbf{Positive to negative attribute similarity}
				& Yes & No  & Yes
				& Avg.\ feature similarity of positive vs negative nodes. \\
				\textbf{Negative to positive attribute similarity}
				& Yes & No  & Yes
				& Avg.\ feature similarity of negative vs positive nodes. \\
				\textbf{Adjusted homophily}
				& Yes & Yes & No
				& Homophily adjusted for degree–class distribution. \\
				\textbf{Node label informativeness}
				& Yes & Yes & No
				& Mutual‐information‐based measure of how informative node labels are about the graph’s connectivity. \\
				\textbf{Balanced accuracy}
				& Yes & Yes & No
				& Class‐averaged fraction of edges connecting nodes to same‐class neighbors. \\
				\textbf{Adjusted accuracy}
				& Yes & Yes & No
				& Accuracy metric adjusted to range $[0,1]$ based on balanced accuracy. \\
				\textbf{Balanced adjusted homophily}
				& Yes & Yes & No
				& Homophily variant balancing class proportions. \\
				\bottomrule
			\end{tabularx}
		}
	\end{table}
\end{frame}

\begin{frame}{Hyperparameter optimization for GNNs}
	We benchmarked 5 standard HPO methods on 9 GNN datasets. The results show that Bayesian optimization (BO) and Tree-structured Parzen Estimators (TPE) achieve the best performance, while Quasi Monte-Carlo (QMC) falls behind.
	\adjustbox{width=\columnwidth}{%
		\input{images/benchmark_ranks/benchmark_ranks.pgf}
	}
\end{frame}

\begin{frame}{The Cross-RF method for HPO on graphs}
	We propose a HPO method using a metamodel \( \mathcal{M}_\rho : \mathfield{R}^d \times \boldsymbol\Lambda \to \mathfield{R} \) that takes as input the properties of a graph dataset \( \delta \left( \mathcal{D} \right) \) and a hyperparameter configuration \( \lambda \), and outputs an estimate of a performance metric \( \rho \). This gives us the following HPO procedure:
	\begin{equation*}
		\tau \left( \mathcal{D}, \mathscr{F}, \tilde{\Lambda}, \rho \right) = \argmax_{\lambda \in \tilde{\Lambda}} \mathcal{M}_\rho \left( \delta \left( \mathcal{D} \right), \lambda \right)
	\end{equation*}
	We show this method to outperform all of the above methods on the 9 graph datasets by a slight margin.
\end{frame}

\end{document}
