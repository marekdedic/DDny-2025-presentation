\documentclass[10pt, aspectratio=169]{beamer}
\setbeamertemplate{navigation symbols}{}

\usepackage{polyglossia}

\usepackage{adjustbox}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{appendixnumberbeamer}
\usepackage[backend=biber,style=iso-authoryear,sortlocale=en_US,autolang=other,bibencoding=UTF8]{biblatex}
\usepackage{booktabs}
\usepackage{color}
\usepackage{csquotes}
\usepackage{datetime}
\usepackage{fontspec}
\usepackage{graphics}
\usepackage{hyperref}
\usepackage{makecell}
\usepackage{mathrsfs}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{pifont}
\usepackage[mode=buildnew]{standalone}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{threeparttable}
\usepackage{todonotes}
\usepackage{ulem}
\usepackage{url}

\addbibresource{zotero.bib}

\setdefaultlanguage{english}
\setotherlanguage{czech}
\setmainfont{TeX Gyre Termes}
\usetheme{Boadilla}
\usecolortheme{crane}
\setbeamertemplate{title page}[default][rounded=true,shadow=false]
\setbeamertemplate{section in toc}[ball unnumbered]
\setbeamertemplate{bibliography item}{}

\hypersetup{%
	pdfencoding=auto,
	unicode=true,
	citecolor=green,
	filecolor=blue,
	linkcolor=red,
	urlcolor=blue
}

\makeatletter
\newcommand*{\currentSection}{\@currentlabelname}
\makeatother

\def\mathdefault#1{#1}

\input{notation}

\title[HPO on for GNNs]
{%
	Benchmarking and Transfer Learning for Hyperparameter Optimization of Graph Neural Networks
}

\newdate{presentation}{25}{11}{2025}
\date[October 2025]{FNSPE Doctoral days, \displaydate{presentation}}

\author[Marek Dědič]
{%
	Marek~Dědič\inst{1, 2}
}

\institute[CTU in Prague, Cisco]
{%
	\inst{1} Czech Technical University in Prague \and
	\inst{2} Cisco Systems, Inc.
}

\begin{document}

\begin{frame}
	\titlepage
\end{frame}

\begin{frame}{Parameters \& hyper-parameters}
	\begin{itemize}
		\item Learners have parameters and hyperparameters
		\item Parameters optimized automatically
		\item Hyper-parameters set by humans
	\end{itemize}
	\centering
	\includegraphics[width=0.5\linewidth]{images/HPO.png}\footcite{kaiku_hyperparameter_2023}
\end{frame}

\begin{frame}{GNN hyper-parameter setups}
	\centering
	\only<1>{\includestandalone[width=\linewidth, page = 1]{images/hyperparameter-setups/hyperparameter-setups}}
	\only<2>{\includestandalone[width=\linewidth, page = 2]{images/hyperparameter-setups/hyperparameter-setups}}
	\only<3>{\includestandalone[width=\linewidth, page = 3]{images/hyperparameter-setups/hyperparameter-setups}}
\end{frame}

\begin{frame}{Hyper-parameter optimization}
	\begin{itemize}
		\item Hyper-parameters need to be found (semi-)automatically
		\item No gradients \( \rightarrow \) black-box problem
		\item \textit{Expensive} black-box problem
	\end{itemize}
\end{frame}

\begin{frame}{HPO loop}
	\centering
	\includegraphics[width=0.8\linewidth]{images/HPO-loop.pdf}\footcite{bischl_hyperparameter_2023}
\end{frame}

\begin{frame}{Hyper-parameter optimization}
	\textbf{Goals \& contributions}:
	\begin{itemize}
		\item Systematic HPO Benchmark on GNNs
		\item Meta-Learning for Hyperparameter Transfer\footnote{Building on previous work \cite{prochazka_which_2023}}
	\end{itemize}
\end{frame}

\begin{frame}{Evaluated standard algorithms}
	\begin{itemize}
		\item Grid search
		\item Random search
		\item Bayesian optimization
		\item Sobol Quasi-Monte Carlo
		\item Tree-structured Parzen Estimator
	\end{itemize}
\end{frame}

\begin{frame}{Experiment setup}
	\begin{itemize}
		\item 9 standard datasets (34 nodes -- 169k nodes)
		\item GraphSAGE model with 8 hyper-parameters
		\item 10 repeated iterations for each configuration
		\item HPO early-stopping based on wall-clock time \& improvement
		\item Used a random forrest regressor as the meta-model
	\end{itemize}
\end{frame}

\begin{frame}{Benchmark results}
	\begin{columns}[t,onlytextwidth]
		\begin{column}{0.45\textwidth}
			\begin{itemize}
				\item SMBO methods (BO and TPE) outperform other methods across all datasets. Between BO and TPE there is no clear winner.
				\item Simple methods like random search and grid search remain solid choices for HPO as they usually trail behind SMBO methods by only a small margin.
				\item Sobol QMC consistently ranks worse than SMBO methods.
			\end{itemize}
		\end{column}

		\begin{column}{0.52\textwidth}
			\centering
			\scriptsize
			\begin{table}
				\centering
				\begin{tabular}{lccccc}
					\toprule
					\textbf{Dataset} & \textbf{Random}    & \textbf{Grid} & \textbf{BO}        & \textbf{TPE}       & \textbf{QMC}       \\
					\midrule
					Cora             & 0.8560             & 0.8491        & \textbf{0.8747}    & \underline{0.8659} & 0.8351             \\
					CiteSeer         & 0.7146             & 0.7046        & \underline{0.7172} & \textbf{0.7236}    & 0.7089             \\
					Squirrel         & \underline{0.3659} & 0.3605        & 0.3544             & \textbf{0.3755}    & 0.3549             \\
					PubMed           & 0.8515             & 0.8457        & \textbf{0.8825}    & \underline{0.8643} & 0.8596             \\
					CoraFull         & 0.6211             & 0.6371        & \underline{0.6450} & \textbf{0.6555}    & 0.6385             \\
					DBLP             & 0.8051             & 0.7996        & \textbf{0.8118}    & 0.8085             & \underline{0.8088} \\
					Computers        & 0.6973             & 0.5999        & \textbf{0.8945}    & \underline{0.8047} & 0.7428             \\
					Flickr           & 0.0864             & 0.0961        & \textbf{0.1908}    & \underline{0.1460} & 0.1069             \\
					ArXiv            & 0.3950             & 0.3796        & \underline{0.3987} & \textbf{0.4098}    & 0.3955             \\
					\bottomrule
				\end{tabular}
				\caption{Final F1 scores for each HPO method and for each dataset, averaged over 10 independent runs. The best method is \textbf{bold} and the second best is \underline{underlined}.}
			\end{table}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Benchmark results -- ranks}
	\begin{center}
		\adjustbox{width=0.8\columnwidth}{%
			\input{images/benchmark_ranks/benchmark_ranks.pgf}
		}
	\end{center}
	Ranks-over-time of the benchmarked HPO methods over the progress of the optimization, aggreagated over 10 independent runs for each of 9 datasets.
\end{frame}

\begin{frame}{Cross-RF approach}
	\begin{itemize}
		\item A \textbf{meta-model}: Predict model performance from its hyper-parameters and dataset properties.
		\item Use for HPO -- selecting the next hyper-parameter configuration \( \hat{\lambda} \):
			\[ \hat{\lambda} = \argmax_{\lambda \in \tilde{\Lambda}} \left( \textrm{meta-model} \left( \lambda \right) \right) \]
		\item Meta-model is cheap \rightarrow \, \( \argmax \) is feasible.
		\item Meta-model can be \textbf{pre-trained} on other similar datasets.
	\end{itemize}
\end{frame}

\begin{frame}{Meta-dataset}
	A meta-dataset used for the pre-training:
	\begin{center}
		\resizebox{\linewidth}{!}{%
			\begin{tabular}{rrrrrrrr}
				\toprule
				\multicolumn{4}{c}{\textbf{Dataset properties}} & \multicolumn{3}{c}{\textbf{Hyper-parameters}} & \textbf{Perf.\ metrics} \\
				\cmidrule(r){1-4} \cmidrule(lr){5-7} \cmidrule(l){8-8}
				\textbf{Node count} & \textbf{Class ratio} & \textbf{\# of components} & \textbf{Avg.\ node degree} & \textbf{\# of layers} & \textbf{Dropout} & \textbf{Activation} & \textbf{F1} \\
				\midrule
				6 & 1 / 3 & 1 & 2 & 2 & 0.3 & ReLU & 0.65 \\
				6 & 1 / 3 & 1 & 2 & 3 & 0.3 & ReLU & 0.7 \\
				6 & 1 / 3 & 1 & 2 & 2 & 0.5 & ReLU & 0.62 \\
				6 & 1 / 3 & 1 & 2 & 3 & 0.5 & ReLU & 0.6 \\
				\bottomrule
			\end{tabular}
		}
	\end{center}
\end{frame}

\begin{frame}{Experiment setup}
	\begin{itemize}
		\item Used a random forrest regressor as the meta-model
		\item Compared with BO and TPE
	\end{itemize}
\end{frame}

\begin{frame}{Meta-model results}
	\begin{columns}[t,onlytextwidth]
		\begin{column}{0.45\textwidth}
			\begin{itemize}
				\item Cross-RF achieves the best final performance on 4 out of the 9 datasets, while being second best on another 3 datasets.
				\item Only on the CiteSeer and CoraFull datasets does Cross-RF perform worse than both reference methods.
				\item Overall, Cross-RF achieves an average rank of 1.78, outperforming both BO (2.0) and TPE (2.22).
			\end{itemize}
		\end{column}

		\begin{column}{0.52\textwidth}
			\begin{table}
				\centering
			\scriptsize
				\begin{tabular}{lccccc}
					\toprule
					\textbf{Dataset} & \textbf{BO}        & \textbf{TPE}       & \textbf{Cross-RF}  \\
					\midrule
					Cora             & \textbf{0.8747}    & 0.8659             & \underline{0.8727} \\
					CiteSeer         & \underline{0.7172} & \textbf{0.7236}    & 0.7095             \\
					Squirrel         & 0.3544             & \underline{0.3755} & \textbf{0.3769}    \\
					PubMed           & \textbf{0.8825}    & 0.8643             & \underline{0.8807} \\
					CoraFull         & \underline{0.6450} & \textbf{0.6555}    & 0.6426             \\
					DBLP             & \underline{0.8118} & 0.8085             & \textbf{0.8123}    \\
					Computers        & \underline{0.8945} & 0.8047             & \textbf{0.9023}    \\
					Flickr           & \underline{0.1908} & 0.1460             & \textbf{0.1956}    \\
					ArXiv            & 0.3987             & \textbf{0.4098}    & \underline{0.4094} \\
					\bottomrule
				\end{tabular}
				\caption{Final F1 scores for each HPO method and for each dataset, averaged over 10 independent runs. The best method is \textbf{bold} and the second best is \underline{underlined}.}
			\end{table}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Meta-model results -- ranks}
	\begin{center}
		\adjustbox{width=0.8\columnwidth}{%
			\input{images/cross-rf_ranks/cross-rf_ranks.pgf}
		}
	\end{center}
	Ranks-over-time of Cross-RF and reference methods over the progress of the optimization, aggreagated over 10 independent runs for each of 9 datasets.
\end{frame}

\begin{frame}{Next steps}
	\begin{itemize}
		\item More performance metrics
		\item Try different meta-model architectures
		\item Try to optimize accross GNN architectures
		\item Training on synthetic graphs
		\item Incorporate as surrogate model for Bayesian HPO
	\end{itemize}
\end{frame}

\begin{frame}{Conclusion}
	\begin{itemize}
		\item Evaluated standard HPO algorithms for GNNs
		\item Shown the Cross-RF hyper-parameter optimization algorithm for GNNs to be comparable to SoTA
		\item Implemented as a standard Optuna sampler
		\item More interesting research to come
	\end{itemize}
\end{frame}

\begin{frame}
	\titlepage
\end{frame}

\appendix

\begin{frame}[allowframebreaks]{Bibliography}
	\printbibliography
\end{frame}

\begin{frame}
	\begin{table}[ht]
		\begin{tabular}{lrrrr}
			\toprule
			Dataset     & \#Nodes       & \#Edges             & \#Features            & \#Classes \\
			\midrule
			KarateClub  & 34                 & 156            & 34                    & 4               \\
			Cora        & 2\,708             & 10\,556        & 1\,433                & 7               \\
			CiteSeer    & 3\,327             & 9\,104         & 3\,703                & 6               \\
			PubMed      & 19\,717            & 88\,648        & 500                   & 3               \\
			ArXiv       & 169\,343           & 1\,166\,243    & 128                   & 40              \\
			DBLP        & 17\,716            & 105\,734       & 1\,639                & 4               \\
			CoraFull    & 19\,793            & 126\,842       & 8\,710                & 70              \\
			Flickr      & 89\,250            & 899\,756       & 500                   & 7               \\
			Computers   & 13\,752            & 491\,722       & 767                   & 10              \\
			Squirrel    & 5\,201             & 198\,493       & 3\,169                & -               \\
			\bottomrule
		\end{tabular}
	\end{table}
\end{frame}

\begin{frame}
	\begin{table}[ht]
		\centering
		\footnotesize
		\setlength{\tabcolsep}{4pt}
		\renewcommand{\arraystretch}{0.75}

		\resizebox{\linewidth}{!}{%
			\begin{threeparttable}
				\begin{tabular}{l c c c c c c c c c c}
					\toprule
					\textbf{Dataset} & \textbf{Ep} & \textbf{Lyrs} & \textbf{HW} & \textbf{Act} & \textbf{Opt} & \textbf{Drop} & \textbf{Agg} & \textbf{ES} & \textbf{L2} & \textbf{LR} \\
					\midrule
					\multicolumn{11}{l}{\itshape Small datasets}\\
					\addlinespace
					KarateClub & 500 & 1--2 & 16--32 &
					\multirow{4}{*}{$\substack{\text{ReLU}\\\text{PReLU}}$} &
					\multirow{4}{*}{$\substack{\text{Adam}\\\text{AdamW}}$} &
					\multirow{4}{*}{$\substack{0 \\ 0.5}$} &
					\multirow{4}{*}{$\substack{\text{Mean}\\\text{Max}}$} & 10 & 0, 1e-3, 1e-2 & 1-e2, 1e-3, 5e-4 \\

					Cora & 200 & 1--3 & 16--64 & & & & & 10 & \multirow{3}{*}{0, 5e-4, 5e-3} & \multirow{3}{*}{1e-2, 5e-3, 1e-3} \\
					CiteSeer & 200 & 1--3 & 16--64 & & & & & 10 &  &  \\
					Squirrel & 500 & 1--3 & 32--128 & & & & & 20 &  &  \\

					\addlinespace
					\multicolumn{11}{l}{\itshape Medium datasets}\\
					\addlinespace
					PubMed & 200 & 1--3 & 16--64 &
					\multirow{4}{*}{$\substack{\text{ReLU}\\\text{PReLU}}$} &
					\multirow{4}{*}{$\substack{\text{Adam}\\\text{AdamW}}$} &
					\multirow{4}{*}{$\substack{0 \\ 0.5}$} &
					\multirow{4}{*}{$\substack{\text{Mean}\\\text{Max}}$} & 10 & 0, 5e-4, 5e-3 & 1e-2, 5e-3, 1e-3 \\

					CoraFull & 500 & 2--3 & 64--128 & & & & & 20 & \multirow{3}{*}{0, 5e-5, 5e-4} & \multirow{3}{*}{1e-2, 5e-3, 1e-3} \\
					DBLP & 500 & 2--3 & 64--128 & & & & & 20 &  & \\
					Computers & 500 & 2--3 & 64--128 & & & & & 20 &  &  \\
					Flickr & 100 & 2--3 & 128--256 & & & & & 10 & 0, 5e-4 & 5e-3, 1e-3 \\

					\addlinespace
					\multicolumn{11}{l}{\itshape Large datasets}\\
					\addlinespace
					ArXiv & 150 & 2--3 & 256--512 &
					$\substack{\text{ReLU}\\\text{PReLU}}$ &
					$\substack{\text{Adam}\\\text{AdamW}}$ &
					$\substack{0 \\ 0.5}$ &
					$\substack{\text{Mean}\\\text{Max}}$ & 10 & 0, 5e-4 & 1e-2, 5e-3 \\
					\bottomrule
				\end{tabular}

				\begin{tablenotes}
					\item[*] \footnotesize\textbf{Abbreviations}: \textbf{Ep}: Epochs, \textbf{Lyrs}: Hidden Layers, \textbf{HW}: Hidden Width, \textbf{Act}: Activation Fn., \textbf{Opt}: Optimizer, \textbf{Drop}: Dropout Rate, \textbf{Agg}: Aggregation Fn., \textbf{ES}: Early Stopping at the model level, \textbf{L2}: L2 Regularization, \textbf{LR}: Learning Rate.
				\end{tablenotes}
			\end{threeparttable}
		}
	\end{table}
\end{frame}

\begin{frame}
	\begin{table}[ht]
		\centering
		\scriptsize
		\renewcommand{\arraystretch}{0.70}
		\resizebox{0.8\linewidth}{!}{%
			\begin{tabularx}{\textwidth}{p{3.7cm} c c c X}
				\toprule
				\makecell[l]{\textbf{Graph Dataset}\\\textbf{Property}}
				& \multicolumn{3}{c}{\textbf{Awareness}}
				& \textbf{Description / Definition} \\
				\cmidrule(lr){2-4}
				& \textbf{Task} & \textbf{Structure} & \textbf{Attributes} & \\
				\midrule
				\textbf{Node count}
				& No  & No  & No
				& Number of nodes — dataset size. \\
				% \textbf{Class ratio}
				%   & Yes & No  & No
				%   & Ratio between the number of positive and negative nodes. \\
				\textbf{Number of components}
				& No  & Yes & No
				& Number of connected components of the graph. \\
				\textbf{Average node degree}
				& No  & Yes & No
				& Average node degree in the graph. \\
				\textbf{Global assortativity}
				& No  & Yes & No
				& Measure of the tendency of nodes to connect with other similar nodes, rather than dissimilar nodes~[9]. \\
				\textbf{Attribute similarity}
				& No  & Yes & Yes
				& Average cosine similarity of attributes across all edges in the graph. \\
				\textbf{Attribute homophily}
				& Yes & Yes & No
				& Measure of how clustered together are nodes with similar attributes. \\
				\textbf{Edge homophily}
				& Yes & Yes & No
				& Fraction of edges connecting nodes of the same class. \\
				\textbf{Node homophily}
				& Yes & Yes & No
				& Fraction of node neighbours having the same class as the node in question, averaged over all nodes. \\
				\textbf{Class homophily}
				& Yes & Yes & No
				& A modification of node homophily that is invariant to the number of classes. \\
				\textbf{Ratio of positive nodes of degree $>1$}
				& Yes & Yes & No
				& Ratio of positive nodes with degree greater than one. \\
				\textbf{Fraction of positive nodes of degree $>2$}
				& Yes & Yes & No
				& The fraction of positive nodes with degree greater than two, out of those with degree greater than one. \\
				\textbf{Average positive node degree}
				& Yes & Yes & No
				& Average node degree in the sub-graph restricted to nodes from $V^1$. \\
				\textbf{Relative presence of positive edges}
				& Yes & Yes & No
				& Number of edges connecting positive nodes, divided by the number of edges that would be present in a theoretical clique constructed of all positive nodes. \\
				\textbf{Positive attribute similarity}
				& Yes & No  & Yes
				& Avg.\ feature similarity within positive class. \\
				\textbf{Positive to negative attribute similarity}
				& Yes & No  & Yes
				& Avg.\ feature similarity of positive vs negative nodes. \\
				\textbf{Negative to positive attribute similarity}
				& Yes & No  & Yes
				& Avg.\ feature similarity of negative vs positive nodes. \\
				\textbf{Adjusted homophily}
				& Yes & Yes & No
				& Homophily adjusted for degree–class distribution. \\
				\textbf{Node label informativeness}
				& Yes & Yes & No
				& Mutual‐information‐based measure of how informative node labels are about the graph’s connectivity. \\
				\textbf{Balanced accuracy}
				& Yes & Yes & No
				& Class‐averaged fraction of edges connecting nodes to same‐class neighbors. \\
				\textbf{Adjusted accuracy}
				& Yes & Yes & No
				& Accuracy metric adjusted to range $[0,1]$ based on balanced accuracy. \\
				\textbf{Balanced adjusted homophily}
				& Yes & Yes & No
				& Homophily variant balancing class proportions. \\
				\bottomrule
			\end{tabularx}
		}
	\end{table}
\end{frame}

\begin{frame}{The Cross-RF method for HPO on graphs}
	We propose a HPO method using a metamodel \( \mathcal{M}_\rho : \mathfield{R}^d \times \boldsymbol\Lambda \to \mathfield{R} \) that takes as input the properties of a graph dataset \( \delta \left( \mathcal{D} \right) \) and a hyperparameter configuration \( \lambda \), and outputs an estimate of a performance metric \( \rho \). This gives us the following HPO procedure:
	\begin{equation*}
		\tau \left( \mathcal{D}, \mathscr{F}, \tilde{\Lambda}, \rho \right) = \argmax_{\lambda \in \tilde{\Lambda}} \mathcal{M}_\rho \left( \delta \left( \mathcal{D} \right), \lambda \right)
	\end{equation*}
	We show this method to outperform all of the above methods on the 9 graph datasets by a slight margin.
\end{frame}

\end{document}
